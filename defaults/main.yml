---
# Copyright 2014, Rackspace US, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

## MaaS Options
#
# maas_auth_method: Specifies how to authenticate against Rackspace Cloud Monitoring.
#
#   This can be set to one of the following two options:
#     - password: authorization uses the variables set in maas_auth_url, maas_api_key and
#                 maas_username
#     - token: authorization uses the maas_api_url and maas_auth_token key
#
maas_auth_method: password

#
# maas_auth_url: Rackspace Cloud auth URL.
#
maas_auth_url: https://identity.api.rackspacecloud.com/v2.0

#
# maas_tenant_id:  Rackspace Cloud tenant ID (required when maas_auth_method is set to password).
#
maas_tenant_id: SomeTenantID

#
# maas_username: Rackspace Cloud user name (required when maas_auth_method is set to password).
#
maas_username: SomeUserName

#
# maas_username: Rackspace Cloud API key (required when maas_auth_method is set to password).
#
maas_api_key: SomeAPIKey

#
# maas_auth_token: Rackspace Cloud auth token (required when maas_auth_method is set to token).
#
maas_auth_token: some_token

#
# maas_api_url: Cloud Monitoring endpoint (required when maas_auth_method is set to token).
#
maas_api_url: https://monitoring.api.rackspacecloud.com/v1.0/{{ maas_tenant_id }}

#
# maas_notification_plan: The Cloud Monitoring notification plan, which defines who will be
#                         notified, and for what.
#
maas_notification_plan: npManaged

#
# maas_external_ip_address:
#
maas_external_ip_address: "{{ external_lb_vip_address }}"

#
# maas_agent_token: The Cloud Monitoring agent token to configure the agent with.
#
#   By default an agent token is created for each entity. However, you may prefer to use the same
#   agent token for all entities. In that case, specify the agent token here.
#
#maas_agent_token: some_token

#
# maas_target_alias: An alias used for remote checks in the event that a device has multiple
#                    public IP addresses or if there are other issues with the default primary IP
#                    address.
#
#   For remote checks, the entity specified by lb_name in rpc_user_config has a variable that
#   denotes the IP addresses to check against. By default, this is the public0_v4 address. Setting
#   this variable enables checks to be performed against public1_v4 or any other variable that is
#   in the entity for the correct IP.
#
maas_target_alias: public0_v4

#
# ssl_check: This flag indicates if the ssl-related checks and alarms should be deployed.
#
ssl_check: false

#
# host_check: This flag indicates if the host-related hardware monitoring checks and alarms should
#             be deployed.
#
host_check: false

#
# remote_check: This flag indicates if the remote.http checks and alarms should be deployed.
#
remote_check: true

#
# lb_name: Defines the entity name of the load balancing device against which the remote MaaS
#          checks and alarms are configured.
#
lb_name: 'unspecified_lb'

#
# maas_scheme: The scheme (http/https) to use when creating remote.http checks.
#
maas_scheme: http

#
# maas_*_scheme: Override the scheme (http/https) for an indivual service.
#
# maas_cinder_scheme: https
# maas_glance_scheme: https
# maas_keystone_scheme: https
# maas_neutron_scheme: https
# maas_nova_scheme: https
maas_horizon_scheme: https
# maas_heat_api_scheme: https
# maas_heat_cfn_scheme: https
# maas_heat_cloudwatch_scheme: https
# maas_swift_proxy_scheme: https

cinder_service_port: 8776
#
# maas_keystone_user: The keystone user to use/create to allow monitoring plugins to query
#                     OpenStack APIs.
#
maas_keystone_user: maas

#
# maas_rabbitmq_user: The rabbitmq user that is created for rabbitmq tests to use.
#
#
maas_rabbitmq_user: maas_user

#
# maas_alarm_local_consecutive_count: The number of consecutive failures before an alert is
#                                     generated for local checks.
#
#   WARNING: Changing this variable affects a customer's SLA, DO NOT change unless you are sure you
#            are sure you know what you're doing!
#
maas_alarm_local_consecutive_count: 3

#
# maas_alarm_remote_consecutive_count: The number of consecutive failures before an alert is
#                                      generate for remote checks.
#
#   WARNING: Changing this variable affects a customer's SLA, DO NOT change unless you are sure you
#            are sure you know what you're doing!
#
maas_alarm_remote_consecutive_count: 1

#
# maas_check_period: The polling interval, defined in seconds
#
#   WARNING: Changing this variable affects a customer's SLA, DO NOT change unless you are sure you
#            are sure you know what you're doing!
#
maas_check_period: 60

#
# maas_check_timeout: Time that will elapse before timeout, defined in seconds. This period must be
#                     less than the maas_check_period.
#
#   WARNING: Changing this variable affects a customer's SLA, DO NOT change unless you are sure you
#            are sure you know what you're doing!
#
maas_check_timeout: 30

#
# maas_monitoring_zones: Specifies the list of data centers (DCs) to poll from for remote
#                        (non-agent) checks.
#
# The default is usually sufficient; for example, if three out of five zones return without failure,
# the check will complete successfully. If for some reason one DC consistently could not perform
# the checks, or if you want to explicitly exclude a DC, set this variable to a list of zones that
# does not include the blocked DC.
#
maas_monitoring_zones:
  - mzdfw
  - mziad
  - mzord
  - mzlon
  - mzhkg

#
# maas_fqdn_extension: Sets the fully-qualified domain name (FQDN) extension that will be appended
#                      to the short names for hosts that are specified in rpc_user_config. This
#                      avoids the need to specify extremely long container names.
#
#   For example, if this variable was not set, a device called 12345-node1.mytestcluster.com would
#   have to be named in openstack_user_config.yml as 12345-node1.mytestcluster.com for MaaS checks
#   to be created correctly. However, if maas_fqdn_extension is set as .mytestcluster.com, the
#   device can be named 12345-node1 in openstack_user_config.yml. This enables shorter container
#   names while still enabling MaaS checks and alarms to be created properly.
#
# maas_fqdn_extension: .example.com
maas_fqdn_extension:

#
# maas_excluded_devices: Specifies devices that will be skipped when creating alarms for disk
#                        utilisation monitoring.
#
# maas_excluded_devices: ['xvde']

#
# maas_filesystem_threshold: Sets the threshold (%) for filesystem monitoring when you are not
#                            specifying specific filesystems.
#
maas_filesystem_warning_threshold: 80.0
maas_filesystem_critical_threshold: 90.0

#
# maas_filesystem_monitors:
#  - filesystem: /
#    warning_threshold: 80.0
#    critical_threshold: 90.0
#  - filesystem: /boot
#    warning_threshold: 80.0
#    critical_threshold: 90.0
_maas_filesystem_monitors: |
  ---
  {% set filesystems = [] %}
  {% for item in ansible_mounts %}
  {%   if 'xfs' in item.fstype or 'ext' in item.fstype %}
  - filesystem: "{{ item.mount }}"
    warning_threshold: "{{ maas_filesystem_warning_threshold }}"
    critical_threshold: "{{ maas_filesystem_critical_threshold }}"
  {%   endif %}
  {% endfor %}

maas_filesystem_monitors: "{{ _maas_filesystem_monitors | from_yaml }}"

#
# maas_filesystem_monitors: Explicitly set the filesystems to set up monitors/alerts for.
#
#   NOTE: You can override these in your openstack_user_config.yml per device using
#         "maas_filesystem_monitors".
#   NOTE(cloudnull): The legacy variable "maas_filesystem_overrides" has been deprecated.
#                    The variable maas_filesystem_monitors can be overridden "as-is".
#                    The variable "maas_filesystem_overrides" will be removed in Q
maas_filesystem_overrides: "{{ maas_filesystem_monitors }}"


# Maas mysql connection thresholds
mysql_connection_warning_threshold: 80
mysql_connection_critical_threshold: 90

# Maas mysql access denied thresholds
# e.g If number of access denied errors inbetween checks
# exceeds 20, the alarm will go into CRITICAL status.
mysql_access_denied_errors_rate_warning_threshold: 10
mysql_access_denied_errors_rate_critical_threshold: 20

# Maas mysql aborted clients rate threshold
# The rate is derived on a per-check basis.
# In other words, this threshold defines how many
# aborted_clients per check are needed to trigger
# an alarm.
mysql_aborted_clients_rate_warning_threshold: 1
mysql_aborted_clients_rate_critical_threshold: 2

# Maas mysql aborted connects rate threshold
# The rate is derived on a per-check basis.
# In other words, this threshold defines how many
# aborted_connects per check are needed to trigger
# an alarm
mysql_aborted_connects_rate_warning_threshold: 1
mysql_aborted_connects_rate_critical_threshold: 2

# Maas mysql open file percentage threshold
mysql_open_files_percentage_warning_threshold: 90
mysql_open_files_percentage_critical_threshold: 95

# Maas InnoDB row lock time avg thresholds, in milliseconds.
innodb_row_lock_time_avg_warning_threshold: 2000
innodb_row_lock_time_avg_critical_threshold: 10000

# Maas InnoDB deadlock thresholds per maas check interval
innodb_deadlocks_rate_warning_threshold: 1
innodb_deadlocks_rate_critical_threshold: 2

# cinder-volumes volume group thresholds
cinder_volumes_vg_warning_threshold: 80.0
cinder_volumes_vg_critical_threshold: 90.0
cinder_vg_name: "{{ cinder_backends['lvm']['volume_group'] | default('cinder-volumes') }}"

# MaaS CPU idle threshold
cpu_idle_percent_avg_warning_threshold: 10.0
cpu_idle_percent_avg_critical_threshold: 1.0

# MaaS Memory in use threshold
memory_used_percentage_warning_threshold: 90.0
memory_used_percentage_critical_threshold: 99.0

# Set the threshold for the "Max Channels per Connection" Rabbitmq alarm
rabbitmq_max_channels_per_con_threshold: 10
rabbitmq_fd_used_threshold: 90
rabbitmq_proc_used_threshold: 90
rabbitmq_socket_used_threshold: 90
rabbitmq_queued_messages_excluding_notifications_threshold: 100

# queued messages per check period (default = 60s)
rabbitmq_queue_growth_rate_threshold: 15


# Set the threshold for the "Disk utilisation" MaaS alarms
disk_utilisation_warning_threshold: 90
disk_utilisation_critical_threshold: 99

# Set the container storage threshold
percent_used_critical_threshold: 85

# nf_conntrack threshold
nf_conntrack_warning_threshold: 80
nf_conntrack_critical_threshold: 90

# net_max_speed has units of Mb/s, set to null for auto discovery
net_max_speed: null
net_rx_pct_warn: 40
net_rx_pct_crit: 60
net_tx_pct_warn: 40
net_tx_pct_crit: 60

# Swift MaaS thresholds
swift_object_quarantine_failed_percentage_threshold: 5.0
swift_object_quarantine_average_threshold: 25.0
swift_object_replication_failure_percentage_threshold: 5.0
swift_object_replication_growth_rate_threshold: 20.0
swift_object_replication_avg_time_threshold: 50.0
swift_account_quarantine_failed_percentage_threshold: 5.0
swift_account_quarantine_average_threshold: 25.0
swift_account_replication_failure_percentage_threshold: 5.0
swift_account_replication_growth_rate_threshold: 10.0
swift_account_replication_avg_time_threshold: 50.0
swift_container_quarantine_failed_percentage_threshold: 5.0
swift_container_quarantine_average_threshold: 25.0
swift_container_replication_failure_percentage_threshold: 5.0
swift_container_replication_growth_rate_threshold: 10.0
swift_container_replication_avg_time_threshold: 50.0
swift_async_pending_failure_percentage_threshold: 5.0
swift_async_pending_average_threshold: 1000.0

# cloud level resource thresholds
cloud_resource_warning_memory: 80.0
cloud_resource_critical_memory: 90.0
cloud_resource_warning_vcpus: 80.0
cloud_resource_critical_vcpus: 90.0
cloud_resource_warning_disk_space: 80.0
cloud_resource_critical_disk_space: 90.0
cloud_resource_cpu_allocation_ratio: 2.0
cloud_resource_mem_allocation_ratio: 1.0

# List of checks, by type - variable:
hp_checks_list:
  - { name: "hp-check", group: "hosts" }

dell_checks_list:
  - { name: "openmanage-memory", group: "hosts" }
  - { name: "openmanage-processors", group: "hosts" }
  - { name: "openmanage-vdisk", group: "hosts" }

disk_utilisation_checks_list:
  - { name: "disk_utilisation", group: "hosts" }

rsyslogd_process_names:
  - rsyslogd

elasticsearch_process_names:
  # NOTE(cjloader): Unlike rsyslog and filebeat, elasticsearch does not have a
  # process name that includes "elasticsearch". The process running
  # elasticsearch is actually named "java" so we search for that here. This is
  # less than ideal, but the best we can do right now.
  - java

filebeat_process_names:
  - filebeat

swift_object_process_names:
  - swift-object-server
  - swift-object-replicator
  - swift-object-updater
  - swift-object-auditor
  - swift-object-expirer

swift_account_process_names:
  - swift-account-server
  - swift-account-replicator
  - swift-account-reaper
  - swift-account-auditor

swift_container_process_names:
  - swift-container-server
  - swift-container-replicator
  - swift-container-updater
  - swift-container-auditor

process_checks_list:
  - { name: "rsyslogd_process_check", group: "rsyslog_all" }
  - { name: "elasticsearch_process_check", group: "elasticsearch_all" }
  - { name: "filebeat_process_check", group: "all" }
  - { name: "swift_object_process_check", group: "swift_obj" }
  - { name: "swift_account_process_check", group: "swift_acc" }
  - { name: "swift_container_process_check", group: "swift_cont" }

ceph_checks_list:
  - { name: "ceph_osd_stats", group: "osds" }
  - { name: "ceph_mon_stats", group: "mons" }
  - { name: "ceph_cluster_stats", group: "mons" }

network_checks_list:
  - name: "{{ ansible_default_ipv4.interface }}"
    group: "hosts"
    max_speed: "{{ net_max_speed }}"
    rx_pct_warn: "{{ net_rx_pct_warn }}"
    rx_pct_crit: "{{ net_rx_pct_crit }}"
    tx_pct_warn: "{{ net_tx_pct_warn }}"
    tx_pct_crit: "{{ net_tx_pct_crit }}"

kernel_checks_list:
  - { name: "conntrack_count", group: "hosts" }

openstack_service_local_checks_list:
  - { name: "cinder_api_local_check", group: "cinder_api" }
  - { name: "cinder_scheduler_check", group: "cinder_scheduler" }
  - { name: "glance_api_local_check", group: "glance_api" }
  - { name: "glance_registry_local_check", group: "glance_registry" }
  - { name: "heat_api_local_check", group: "heat_api" }
  - { name: "heat_cfn_api_check", group: "heat_api_cfn" }
  - { name: "heat_cw_api_check", group: "heat_api_cloudwatch" }
  - { name: "horizon_local_check", group: "horizon" }
  - { name: "keystone_api_local_check", group: "keystone" }
  - { name: "neutron_api_local_check", group: "neutron_server" }
  - { name: "neutron_dhcp_agent_check", group: "neutron_dhcp_agent" }
  - { name: "neutron_l3_agent_check", group: "neutron_l3_agent" }
  - { name: "neutron_linuxbridge_agent_check", group: "neutron_linuxbridge_agent" }
  - { name: "neutron_metadata_agent_check", group: "neutron_metadata_agent" }
  - { name: "neutron_metering_agent_check", group: "neutron_metering_agent" }
  - { name: "nova_api_metadata_local_check", group: "nova_api_metadata" }
  - { name: "nova_api_local_check", group: "nova_api_os_compute" }
  - { name: "nova_compute_check", group: "nova_compute" }
  - { name: "nova_cert_check", group: "nova_cert" }
  - { name: "nova_conductor_check", group: "nova_conductor" }
  - { name: "nova_scheduler_check", group: "nova_scheduler" }
  - { name: "nova_consoleauth_check", group: "nova_console" }
  - { name: "nova_console_check", group: "nova_console" }
  - { name: "nova_cloud_stats_check", group: "nova_api_os_compute" }

magnum_checks_list:
  - { name: "magnum_api_local_check", group: "magnum" }
  - { name: "magnum_conductor_check", group: "magnum" }

cinder_vg_checks_list:
  - { name: "cinder_vg_check", group: "cinder_volume", cinder_vg_name: "{{ cinder_vg_name }}" }

#
# cinder_backup_checks_list: A list of checks for the cinder-backup service
#
#   This check was originally part of openstack_service_local_checks_list, but
#   as cinder-backup is enabled/disabled via
#   cinder_service_backup_program_enabled in the os_cinder role we have pulled
#   it out so we can make a separate task conditional on that variable.
#
cinder_backup_checks_list:
  - { name: "cinder_backup_check", group: "cinder_backup" }

swift_checks_list:
  - { name: "swift_account_server_check", group: "swift_acc" }
  - { name: "swift_container_server_check", group: "swift_cont" }
  - { name: "swift_object_server_check", group: "swift_obj" }

swift_recon_checks_list:
  - { name: "swift_account_replication_check", group: "swift_proxy" }
  - { name: "swift_async_check", group: "swift_proxy" }
  - { name: "swift_container_replication_check", group: "swift_proxy" }
  - { name: "swift_md5_check", group: "swift_proxy" }
  - { name: "swift_object_replication_check", group: "swift_proxy" }
  - { name: "swift_proxy_server_check", group: "swift_proxy" }
  - { name: "swift_quarantine_check", group: "swift_proxy" }
  - { name: "swift_time_sync_check", group: "swift_proxy" }

openstack_service_remote_checks_list:
  - { name: "lb_api_check_cinder", group: "cinder_api" }
  - { name: "lb_api_check_glance", group: "glance_api" }
  - { name: "lb_api_check_keystone", group: "keystone" }
  - { name: "lb_api_check_neutron", group: "neutron_server" }
  - { name: "lb_api_check_nova", group: "nova_api_os_compute" }
  - { name: "lb_api_check_horizon", group: "horizon" }
  - { name: "lb_api_check_heat_api", group: "heat_api" }
  - { name: "lb_api_check_heat_cfn", group: "heat_api_cfn" }
  - { name: "lb_api_check_heat_cloudwatch", group: "heat_api_cloudwatch" }
  - { name: "lb_api_check_swift_healthcheck", group: "swift_proxy" }
  - { name: "lb_api_check_swift_access", group: "swift_proxy" }

ssl_cert_checks_list:
  - { name: "lb_ssl_cert_expiry_check", group: "utility"}

infra_service_local_checks_list:
  - { name: "rabbitmq_status", group: "rabbitmq" }
  - { name: "galera_check", group: "galera" }
  - { name: "memcached_status", group: "memcached" }

holland_service_local_checks_list:
  - { name: "holland_local_check", group: "galera" }

# Set this to 'false' to set the lb checks against ALL service infra hosts.
# Checks will be setup but disabled on all but host[0] when this is 'true'.
lb_check_on_single_host: true

raxmon_repo_url: "http://stable.packages.cloudmonitoring.rackspace.com/ubuntu-{{ansible_distribution_version}}-x86_64"

maas_apt_keys:
  - { url: "https://monitoring.api.rackspacecloud.com/pki/agent/linux.asc", state: "present" }

maas_apt_repos:
  - { repo: "deb {{ raxmon_repo_url }} cloudmonitoring main", state: "present" }

maas_apt_packages:
  - rackspace-monitoring-agent
  - mariadb-client

#
# maas_requires_pip_packages: Pip packages needed outside the virtualenv
#
maas_requires_pip_packages:
  - virtualenv

#
# maas_pip_packages: These packages are installed inside the virtualenv.
#
maas_pip_packages:
  - cryptography
  - ipaddr
  - lxml
  - psutil
  - apache-libcloud<2.0.0
  - rackspace-monitoring-cli
  - openstacksdk
  - python-cinderclient
  - python-glanceclient
  - python-heatclient
  - python-keystoneclient
  - python-magnumclient
  - python-neutronclient
  - python-novaclient
  - python-memcached
  - requests
  - swift
  - waxeye

#
# maas_pip_container_packages: install these packages only on hosts with containers
#
maas_pip_container_packages:
  - lxc-python2

maas_source_plugin_dir: plugins/

maas_plugin_dir: /usr/lib/rackspace-monitoring-agent/plugins/

# The following two variables control which checks and alarms are excluded from
# MaaS. Here is explanation of what each variable does:
#
# 1) When you add a check to `maas_excluded_checks`, the check (and any
#    associated alarms) are not provisioned. This disables graphing,
#    monitoring, alarms and alerts for that particular check.
#
# 2) When you add an alarm to `maas_excluded_alarms`, the alarm is not
#    provisioned. However, the check itself (and any associated graphing) is
#    maintained.
#
# Both lists can contain simple strings that match exactly, or they can contain
# regular expressions.
maas_excluded_checks: []
maas_excluded_alarms: []

# Customizing MaaS check period and timeout.
# The following two variables allow deployers to provide custom check periods
# and timeouts for certain checks. Each variable is a list of dictionaries
# containing the check label and the appropriate value:
#
#   maas_check_period_override:
#     - disk_utilization: 60
#     - conntrack_count: 45
#   maas_check_timeout_override:
#     - cinder_backup_check: 120
#
maas_check_period_override: {}
maas_check_timeout_override: {}

# openrc definitions from OSA
# This is necessary until LP #1537117 is implemented
openrc_os_endpoint_type: internalURL
openrc_insecure: "{{ (keystone_service_adminuri_insecure | bool or keystone_service_internaluri_insecure | bool) | default(false) }}"
# Name of the virtual env to deploy into (maas)
maas_venv: "/openstack/venvs/maas-{{ rpc_release }}"
maas_venv_bin: "{{ maas_venv }}/bin"

#
# Default horizon site name
#
horizon_site_name: "openstack dashboard"

#
# maas_monitor_cinder_backup: This variable determines if the check for the
#                             cinder-backup service should be deployed
#
maas_monitor_cinder_backup: false

#
# verify_maas_delay: How long to wait between failures if there is one
#
verify_maas_delay: 20

#
# verify_maas_retries: How many times to retry on failure
#
verify_maas_retries: 5

#
# maas_use_api: Allow operations that make use of the MaaS api, set to false
#               for offline testing
#
maas_use_api: false

#
# maas_agent_upgrade: Allow for automatic MaaS agent upgrades
#
#
maas_agent_upgrade: true


# Set the package install state for distribution and pip packages
# Options are 'present' and 'latest'
maas_package_state: "latest"
maas_pip_package_state: "latest"

# Swift object access check
swift_accesscheck_enabled: true
swift_accesscheck_container: "accesscheck"
swift_accesscheck_file: "index.html"
swift_accesscheck_user_name: "accesscheck"
swift_operator_role: "swiftoperator"
swift_service_project_name: "service"

# Instruct the swift service checks to be ldap aware.
swift_service_in_ldap: false

# Holland venv enabled
holland_venv_enabled: true
holland_venv_bin: "/openstack/venvs/holland-{{ rpc_release }}/bin"

internal_vip_address: "{{ internal_lb_vip_address }}"

# Set the openstack-release to testing by default
openstack_release: testing
